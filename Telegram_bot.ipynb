{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HUnLSX_bK-fR"
      },
      "outputs": [],
      "source": [
        "# Описание поставленной задачи\n",
        "\n",
        "# Требуется удалить фон у центрального объекта изображения, то есть необходимо осуществить задачу сегментации изображения, где каждый пиксель будет \n",
        "# классифицирован в рамках \"фон\" или \"объект\", с дальнейший наложением белой маски на изображение там, где отсутствует объект.\n",
        "\n",
        "# Следовательной, необходимо в первую очередь решить проблему сегмантации и подобрать соотвутствующие данные для дальнейшего обучения нейронной сети."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "coKDMAjWuEak"
      },
      "outputs": [],
      "source": [
        "# Описание Датасетов\n",
        "\n",
        "# 1. Cityscapes Image Pairs\n",
        "# Данные о городских пейзажах (домашняя страница набора данных) содержат помеченные видео, \n",
        "# снятые с транспортных средств, управляемых в Германии. \n",
        "# Эта версия представляет собой обработанную подвыборку, созданную как часть документа Pix2Pix. \n",
        "# Набор данных содержит неподвижные изображения из исходных видеороликов, \n",
        "# а метки семантической сегментации отображаются на изображениях рядом с исходным изображением. \n",
        "# Это один из лучших наборов данных для задач семантической сегментации.\n",
        "# Этот набор данных содержит 2975 файлов обучающих изображений и 500 файлов проверочных изображений. \n",
        "# Каждый файл изображения имеет размер 256x512 пикселей,\n",
        "# и каждый файл представляет собой композит с исходной фотографией в левой половине изображения, \n",
        "# рядом с помеченным изображением (результат семантической сегментации) в правой половине.\n",
        "\n",
        "# https://www.kaggle.com/datasets/dansbecker/cityscapes-image-pairs\n",
        "\n",
        "# 2.The COCO-Stuff dataset\n",
        "# COCO-Stuff дополняет все 164K изображений популярного набора данных COCO [2] аннотациями на уровне пикселей. \n",
        "# Эти аннотации можно использовать для задач понимания сцены, таких как семантическая сегментация, обнаружение объектов и субтитры к изображениям.\n",
        "# Все лучшие собранные датасеты COCO-Stuff со статистикой.\n",
        "# https://github.com/nightrome/cocostuff\n",
        "\n",
        "# 3. ADE20K\n",
        "# Набор данных ADE20K содержит более 20 тысяч изображений, ориентированных на сцены, с исчерпывающими комментариями к объектам и частям объектов. \n",
        "# В частности, бенчмарк разделен на 20 тыс. изображений для обучения, 2 тыс. изображений для проверки и еще одну партию готовых изображений для тестирования. \n",
        "# Всего для оценки включено 150 семантических категорий, которые включают такие предметы, как небо, дорога, трава, а также \n",
        "# отдельные объекты, такие как человек, автомобиль, кровать. Обратите внимание, что на изображениях наблюдается неравномерное распределение объектов, \n",
        "# имитирующее более естественное появление объектов в повседневной сцене.\n",
        "\n",
        "# http://sceneparsing.csail.mit.edu/\n",
        "\n",
        "# 4. PASCAL Visual Object Classes Challenge 2012\n",
        "# 20 классов. Данные train/val содержат 11 530 изображений, содержащих 27 450 объектов с аннотацией ROI и 6 929 сегментов.\n",
        "# http://host.robots.ox.ac.uk:8080/pascal/VOC/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1gTeV5LnsYZy"
      },
      "outputs": [],
      "source": [
        "# Описание всех текущих реализаций\n",
        "\n",
        "# 1.Image-Segmentation-with-UNet\n",
        "# Решение задачи сегментации изображения уличной среды на примере нейросети UNet с помощью датасета Cityscapes\n",
        "# https://www.kaggle.com/code/gokulkarthik/image-segmentation-with-unet-pytorch/notebook\n",
        "\n",
        "# 2.Introduction to Semantic Segmentation with fcn resnet_101\n",
        "# Решение задачи сегментации изображения на примере сравнения двух нейросетей - FCN и DeepLabv3 \n",
        "# https://colab.research.google.com/github/spmallick/learnopencv/blob/master/PyTorch-Segmentation-torchvision/intro-seg.ipynb#scrollTo=KMdbwOEbeQ0s\n",
        "\n",
        "# 3.Instance segmentation with maskrcnn_resnet50_fpn\n",
        "# Испоьзование готовой предобученной модели для решения задачи сегментации\n",
        "# https://colab.research.google.com/drive/1b3TwHdeWAgmZ7n1eOFbzVw1T0oZvC9U6?usp=sharing#scrollTo=sPwMdED4tJ9M\n",
        "\n",
        "# 4. Semantic Segmentation\n",
        "# Содержит:\n",
        "# (Deeplab V3+) Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation\n",
        "# (GCN) Large Kernel Matter, Improve Semantic Segmentation by Global Convolutional Network\n",
        "# (UperNet) Unified Perceptual Parsing for Scene Understanding\n",
        "# (DUC, HDC) Understanding Convolution for Semantic Segmentation\n",
        "# (PSPNet) Pyramid Scene Parsing Network\n",
        "# (ENet) A Deep Neural Network Architecture for Real-Time Semantic Segmentatio\n",
        "# (U-Net) Convolutional Networks for Biomedical Image Segmentation (2015)\n",
        "# (SegNet) A Deep ConvolutionalEncoder-Decoder Architecture for ImageSegmentation (2016)\n",
        "# (FCN) Fully Convolutional Networks for Semantic Segmentation (2015)\n",
        "# https://github.com/yassouali/pytorch-segmentation#models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TKOX4H8PsYtQ"
      },
      "outputs": [],
      "source": [
        "# Подход к обработки данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 571
        },
        "id": "0Hh8fqpcsYv1",
        "outputId": "fd0daea3-739e-4442-9acb-00c2b65b4676"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyTelegramBotAPI\n",
            "  Downloading pyTelegramBotAPI-4.6.0.tar.gz (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 13.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pyTelegramBotAPI) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pyTelegramBotAPI) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pyTelegramBotAPI) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pyTelegramBotAPI) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pyTelegramBotAPI) (3.0.4)\n",
            "Building wheels for collected packages: pyTelegramBotAPI\n",
            "  Building wheel for pyTelegramBotAPI (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyTelegramBotAPI: filename=pyTelegramBotAPI-4.6.0-py3-none-any.whl size=140743 sha256=93ebefd297647bb26691bab9633436abf625d1c2a69b3b07cfd8c7c310e9a7a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/97/44/d71f8ee4149d87c457cef044f84d9b40794202b88358ad94dc\n",
            "Successfully built pyTelegramBotAPI\n",
            "Installing collected packages: pyTelegramBotAPI\n",
            "Successfully installed pyTelegramBotAPI-4.6.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-684c6e47de81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install pyTelegramBotAPI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[0;31m# quantization depends on torch.fx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[0;31m# Import quantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 905\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mquantization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m \u001b[0;31m# Import the quasi random sampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mobserver\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mqconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfake_quantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfuse_modules\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfuse_modules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/quantization/quantize.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \"\"\"\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_observer_forward_hook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mao\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_propagate_qconfig_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/ao/quantization/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquant_type\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantization_mappings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mquantize_jit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mstubs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m  \u001b[0;31m# noqa: F403\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!pip install pyTelegramBotAPI\n",
        "from torchvision import models\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as pli\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torchvision.transforms as T\n",
        "from io import BytesIO\n",
        "from skimage import io\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9-Se5r7sYys"
      },
      "outputs": [],
      "source": [
        "import telebot\n",
        "token = '5331173165:AAHt7PEr72frv6QNslq5gc1jdEYCmL_59is'\n",
        "bot = telebot.TeleBot(token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYuwWNpLLlzT"
      },
      "outputs": [],
      "source": [
        "@bot.message_handler(content_types=['text'])\n",
        "\n",
        "def get_text_messages(message):\n",
        "  if message.text == \"Привет\":\n",
        "    send = bot.send_message(message.from_user.id, \"Привет, я занимаюсь удалением фона с картинки, но только у определенных обьектов. Весь список объектов можно получить в /help. Все что нужно это просто отправить мне фотографию в формате jpeg.\")\n",
        "    bot.register_next_step_handler(send, handle_docs_document)\n",
        "  elif message.text == \"/help\":\n",
        "    bot.send_message(message.from_user.id, \"1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle, 6=bus, 7=car, 8=cat, 9=chair, 10=cow, 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person, 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor.\")\n",
        "  else:\n",
        "    bot.send_message(message.from_user.id, \"Я тебя не понимаю. \")\n",
        "\n",
        "def handle_docs_document(message):\n",
        "    if message.content_type == 'photo':\n",
        "      # print ('message.photo =', message.photo)\n",
        "      # fileID = message.photo[-1].file_id\n",
        "      # print ('fileID =', fileID)\n",
        "      # file = bot.get_file(fileID)\n",
        "      # print ('file.file_path =', file.file_path)\n",
        "      # bot.send_message(message.from_user.id, \"Картинку получил \")\n",
        "      # img = file.file_path\n",
        "      # new_photo = Delete_Background(img)\n",
        "      # bot.send_photo(message.from_user.id, photo=open(new_photo, 'rb'))\n",
        "      fileID = message.photo[-1].file_id\n",
        "      # print ('fileID =', fileID)\n",
        "      file_info = bot.get_file(fileID)\n",
        "      # print ('file.file_path =', file_info.file_path)\n",
        "      downloaded_file = bot.download_file(file_info.file_path)\n",
        "      print(downloaded_file)\n",
        "      bot.send_message(message.from_user.id, \"Картинку получил, обрабатываю... \")\n",
        "      new_photo = Delete_Background(downloaded_file)\n",
        "\n",
        "      pli.imsave('name.png', new_photo)\n",
        "\n",
        "      bot.send_photo(message.from_user.id, photo=open('name.png', 'rb'))\n",
        "\n",
        "def Delete_Background(img):\n",
        "    # Define the helper function\n",
        "  def decode_segmap(image1, nc=21):\n",
        "    # Load the foreground input image\n",
        "    photo=BytesIO(img)\n",
        "    foreground = io.imread(photo)\n",
        "    size=np.array(image1.shape)\n",
        "    print(size)\n",
        "    foreground = cv2.resize(foreground, (size[1], 450))\n",
        "    print(image1.shape)\n",
        "    # Change the color of foreground image to RGB\n",
        "    label_colors = np.array([(0, 0, 0),  # 0=background\n",
        "                              # 1=aeroplane, 2=bicycle, 3=bird, 4=boat, 5=bottle\n",
        "                              (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128), (128, 0, 128),\n",
        "                              # 6=bus, 7=car, 8=cat, 9=chair, 10=cow\n",
        "                              (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0), (64, 128, 0),\n",
        "                              # 11=dining table, 12=dog, 13=horse, 14=motorbike, 15=person\n",
        "                              (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128), (192, 128, 128),\n",
        "                              # 16=potted plant, 17=sheep, 18=sofa, 19=train, 20=tv/monitor\n",
        "                              (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0), (0, 64, 128)])\n",
        "\n",
        "    r = np.zeros_like(image1).astype(np.uint8)\n",
        "    g = np.zeros_like(image1).astype(np.uint8)\n",
        "    b = np.zeros_like(image1).astype(np.uint8)\n",
        "      \n",
        "    for l in range(0, nc):\n",
        "          idx = image1 == l\n",
        "          r[idx] = label_colors[l, 0]\n",
        "          g[idx] = label_colors[l, 1]\n",
        "          b[idx] = label_colors[l, 2]\n",
        "\n",
        "    rgb = np.stack([r, g, b], axis=2)\n",
        "    print(rgb.shape)\n",
        "    pic = foreground.astype(np.uint8)\n",
        "    # Create a binary mask of the RGB output map using the threshold value 0\n",
        "    th, alpha = cv2.threshold(np.array(rgb), 0, 255, cv2.THRESH_BINARY)\n",
        "\n",
        "    plt.imshow(alpha)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "    resized = cv2.resize(foreground, (foreground.shape[1],foreground.shape[0]))\n",
        "    print(\"Разделение\")\n",
        "   \n",
        "    # Apply a slight blur to the mask to soften edges\n",
        "    alpha = cv2.GaussianBlur(alpha, (7, 7), 0)\n",
        "\n",
        "    alpha =cv2.cvtColor(alpha, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    print(resized.shape)\n",
        "    print(alpha.shape)\n",
        "\n",
        "    #print(type(alpha[0][0][0]))\n",
        "    #print(type(resized[0][0][0]))\n",
        "\n",
        "    out_img = cv2.bitwise_and(resized,resized,mask = alpha)\n",
        "\n",
        "    plt.imshow(alpha)\n",
        "    plt.imshow(out_img)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    return out_img/255\n",
        "\n",
        "\n",
        "  def segment(net, img1, show_orig=True, dev='cuda'):\n",
        "      #img = Image.open(path)\n",
        "      if show_orig:\n",
        "          plt.imshow(img1)\n",
        "          plt.axis('off')\n",
        "          plt.show()\n",
        "      # Comment the Resize and CenterCrop for better inference results\n",
        "      trf = T.Compose([T.Resize(450),\n",
        "                      #T.CenterCrop(450),\n",
        "                      T.ToTensor(),\n",
        "                      T.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                  std=[0.229, 0.224, 0.225])])\n",
        "      \n",
        "      dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "      inp = trf(img1).unsqueeze(0).to(dev)\n",
        "      out = net.to(dev)(inp)['out']\n",
        "      om = torch.argmax(out.squeeze(), dim=0).detach().cpu().numpy()\n",
        "\n",
        "      rgb = decode_segmap(om)\n",
        "\n",
        "      # plt.imshow(rgb)\n",
        "      # plt.axis('off')\n",
        "      # plt.show()\n",
        "      # bot.send_photo(message.from_user.id, photo=open(rgb, 'rb'))\n",
        "      return rgb\n",
        "  dlab = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()\n",
        "  image=Image.open(BytesIO(img))\n",
        "  rgb = segment(dlab, image, show_orig=True)\n",
        "  return rgb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMuZyWJ-NtsC"
      },
      "source": [
        "# Новый раздел"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVkBd4T9LzDZ"
      },
      "outputs": [],
      "source": [
        "bot.polling(none_stop=True, interval=0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Work_test_duty_last.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}